{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fde42a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = './Positive_data'\n",
    "TEST_PATH = './test'\n",
    "ckpt_dir = './pretrained/efficientdet_d5-ef44aea8.pth'\n",
    "PREDICTION_THRES = 0.8\n",
    "EPOCHS = 5\n",
    "MIN_SIZE = 800\n",
    "BATCH_SIZE = 2\n",
    "DEBUG = False # to visualize the images before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0303117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# sci-kit learn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# etc\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5501d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from effdet.config.model_config import efficientdet_model_param_dict\n",
    "# from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "# from effdet.efficientdet import HeadNet\n",
    "# from effdet.efficientdet import HeadNet\n",
    "# from effdet.config.model_config import efficientdet_model_param_dict\n",
    "\n",
    "\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37345615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'number of configs: {len(efficientdet_model_param_dict)}')\n",
    "\n",
    "# list(efficientdet_model_param_dict.keys())[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4611242",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "def create_model(num_classes=1, image_size=512, architecture=\"tf_efficientdet_d5\"):\n",
    "#     efficientdet_model_param_dict['tf_efficientdet_d5'] = dict(\n",
    "#         name='tf_efficientdet_d5',\n",
    "#         backbone_name='tf_efficientdet_d5',\n",
    "#         backbone_args=dict(drop_path_rate=0.2),\n",
    "#         num_classes=num_classes,\n",
    "#         url='', )\n",
    "\n",
    "    config = get_efficientdet_config(architecture)\n",
    "    config.update({'num_classes': num_classes})\n",
    "    config.update({'image_size': (image_size, image_size)})\n",
    "\n",
    "    #print(config)\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    net.class_net = HeadNet(\n",
    "        config,\n",
    "        num_outputs=config.num_classes,\n",
    "    )\n",
    "    return DetBenchTrain(net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b527e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(ckpt_path):\n",
    "    net = create_model(num_classes=1, image_size=512, architecture='tf_efficientdet_d5')\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    net.load_state_dict(checkpoint, strict=False)\n",
    "    \n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba821969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotHoleDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        image = cv2.imread(f\"{self.image_dir}/{image_id}.JPG\", cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "    \n",
    "        # convert the boxes into x_min, y_min, x_max, y_max format\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        # we have only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)   \n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['img_scale'] = torch.tensor([1.])\n",
    "        target['image_id'] = torch.tensor([index])  # It's not important >_<\n",
    "        target['img_size'] = torch.tensor([(512, 512)])\n",
    "\n",
    "        \n",
    "        \n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            while True:               \n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                }\n",
    "                sample = self.transforms(**sample)\n",
    "\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    # EfficientNet implementation in the effdet library takes bboxes in yxyx format\n",
    "                    target['boxes'] = torch.tensor(sample['bboxes'])\n",
    "                    target['boxes'][:, [0, 1, 2, 3]] = target['boxes'][:, [1, 0, 3, 2]]\n",
    "                    target['labels'] = torch.stack(sample['labels']) # have to add this\n",
    "                    break\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27294797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function helps when we have different number of object instances\n",
    "    in the batches in the dataset.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7292f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the image transforms\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        # A.RandomRotate90(0.5),\n",
    "        # A.MotionBlur(p=0.2),\n",
    "        # A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        # A.Blur(blur_limit=3, p=0.1),\n",
    "        A.Resize(height=512, width=512, p=1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "734b26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_id  num_potholes     x     y    w   h\n",
      "0  G0010033             6  1990  1406   66  14\n",
      "1  G0010033             6  1464  1442   92  16\n",
      "2  G0010033             6  1108  1450   54  16\n",
      "3  G0010033             6   558  1434  102  16\n",
      "4  G0010033             6   338  1450   72  18\n",
      "Total number of image IDs (objects) in dataframe: 4592\n",
      "Total number of training images in folder: 1119\n",
      "Total number of unique train images IDs in dataframe: 1337\n",
      "Number of image IDs (objects) training on: 3896\n"
     ]
    }
   ],
   "source": [
    "# path to the input root directory\n",
    "DIR_INPUT = ROOT_PATH\n",
    "# read the annotation CSV file\n",
    "train_df = pd.read_csv(\"./_df.csv\")\n",
    "print(train_df.head())\n",
    "print(f\"Total number of image IDs (objects) in dataframe: {len(train_df)}\")\n",
    "# get all the image paths as list\n",
    "image_paths = glob.glob(f\"{DIR_INPUT}/*.JPG\")\n",
    "image_names = []\n",
    "for image_path in image_paths:\n",
    "    image_names.append(image_path.split(os.path.sep)[-1].split('.')[0])\n",
    "print(f\"Total number of training images in folder: {len(image_names)}\")\n",
    "image_ids = train_df['image_id'].unique()\n",
    "print(f\"Total number of unique train images IDs in dataframe: {len(image_ids)}\")\n",
    "# number of images that we want to train out of all the unique images\n",
    "train_ids = image_names[:] # use all the images for training\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]\n",
    "print(f\"Number of image IDs (objects) training on: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6d7e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PotHoleDataset(train_df, DIR_INPUT, train_transform())\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a536d05c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FeatureInfo' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3823/3716053922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the computation device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3823/2917336176.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(ckpt_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_efficientdet_d5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3823/92891698.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(num_classes, image_size, architecture)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#print(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientDet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     net.class_net = HeadNet(\n\u001b[1;32m     18\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/5.skku/effdet/efficientdet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, norm_kwargs, pretrained_backbone)\u001b[0m\n\u001b[1;32m    432\u001b[0m             pretrained=pretrained_backbone, **config.backbone_args)\n\u001b[1;32m    433\u001b[0m         feature_info = [dict(num_chs=f['num_chs'], reduction=f['reduction'])\n\u001b[0;32m--> 434\u001b[0;31m                         for i, f in enumerate(self.backbone.feature_info())]\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiFpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHeadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'FeatureInfo' object is not callable"
     ]
    }
   ],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model(ckpt_dir).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch, (images, targets, image_ids) in enumerate(train_dataloader, 1):\n",
    "\n",
    "#     for i, data in enumerate(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # images, targets, images_ids = data[0], data[1], data[2]\n",
    "        \n",
    "        \n",
    "        # images = list(image.to(device) for image in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        images = images.to(device).float()\n",
    "        boxes = [target['boxes'].to(device).float() for target in targets]\n",
    "        labels = [target['labels'].to(device).float() for target in targets]\n",
    "        img_scale = torch.tensor([target['img_scale'].to(device).float() for target in targets])\n",
    "        img_size = torch.tensor([(512, 512) for _ in targets]).to(device).float()\n",
    "\n",
    "        target_res = {}\n",
    "        target_res['bbox'] = boxes\n",
    "        target_res['cls'] = labels\n",
    "        target_res['img_scale'] = img_scale\n",
    "        target_res['img_size'] = img_size\n",
    "\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images, target_res)\n",
    "        \n",
    "        \n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 25 == 0:\n",
    "            print(f\"Iteration #{batch} loss: {loss}\")\n",
    "    train_loss = running_loss/len(train_dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    torch.save(model.state_dict(), 'checkpoint/effdet_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    \"\"\"\n",
    "    This function will only execute if `DEBUG` is `True` in \n",
    "    `config.py`.\n",
    "    \"\"\"\n",
    "    images, targets, image_ids = next(iter(train_data_loader))\n",
    "    images = list(image for image in images)\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    for i in range(1):\n",
    "        boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "        sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "        for box in boxes:\n",
    "            cv2.rectangle(sample,\n",
    "                        (box[0], box[1]),\n",
    "                        (box[2], box[3]),\n",
    "                        (220, 0, 0), 3)\n",
    "        ax.set_axis_off()\n",
    "        plt.imshow(sample)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss = train(train_data_loader)\n",
    "    print(f\"Epoch #{epoch} loss: {train_loss}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {(end - start) / 60} minutes for epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e4053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43da99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the computation device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# load the model and the trained weights\n",
    "model =create_model(num_classes=1, image_size=512)\n",
    "model.load_state_dict(torch.load('checkpoint/effdet_test.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TEST = TEST_PATH\n",
    "test_images = os.listdir(DIR_TEST)\n",
    "print(f\"Validation instances: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = PREDICTION_THRES\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, image in tqdm(enumerate(test_images), total=len(test_images)):\n",
    "        orig_image = cv2.imread(f\"{DIR_TEST}/{test_images[i]}\", cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        # make the pixel range between 0 and 1\n",
    "        image /= 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float)\n",
    "        image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        cpu_device = torch.device(\"cpu\")\n",
    "       \n",
    "\n",
    "        target_res = {}\n",
    "        target_res['bbox'] = boxes\n",
    "        target_res['cls'] = labels\n",
    "        target_res['img_scale'] = img_scale\n",
    "        target_res['img_size'] = img_size\n",
    "        \n",
    "         outputs = model(image, target_res)[\"detections\"]\n",
    "        \n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        if len(outputs[0]['boxes']) != 0:\n",
    "            for counter in range(len(outputs[0]['boxes'])):\n",
    "                boxes = outputs[0]['boxes'].data.numpy()\n",
    "                scores = outputs[0]['scores'].data.numpy()\n",
    "                boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "                draw_boxes = boxes.copy()\n",
    "                \n",
    "            for box in draw_boxes:\n",
    "                cv2.rectangle(orig_image,\n",
    "                            (int(box[0]), int(box[1])),\n",
    "                            (int(box[2]), int(box[3])),\n",
    "                            (0, 0, 255), 3)\n",
    "                cv2.putText(orig_image, 'PotHole', \n",
    "                            (int(box[0]), int(box[1]-5)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), \n",
    "                            2, lineType=cv2.LINE_AA)\n",
    "            cv2.imwrite(f\"test_predictions/{test_images[i]}\", orig_image,)\n",
    "print('TEST PREDICTIONS COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10ed26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c51481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23df90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bd4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808bfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test3_p3.9]",
   "language": "python",
   "name": "conda-env-test3_p3.9-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
